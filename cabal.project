packages:
    .
library-stripping: True
executable-stripping: True
optimization: 2
executable-profiling: False
library-profiling: False
profiling: False
profiling-detail: none
allow-newer: all
jobs: $ncpus
semaphore: True
split-sections: True
executable-dynamic: True
program-options
    ghc-options:
                 -- LLVM options
                 -fllvm
                 -mavx
                 -mavx2
                 -mbmi
                 -mbmi2
                 -msse
                 -msse2
                 -msse3
                 -msse4
                 -msse4.2
                 -mavx512cd
                 -mavx512f
                 -pgmlas=clang
                 -pgmc=clang
                 -pgml=clang
                 -pgmlo=opt
                 -pgmlc=llc
                 -opta-mcpu=znver4
                 -optl-mcpu=znver4
                 -optlo-mcpu=znver4
                 -optlc-mcpu=znver4
                 -optlas-mcpu=znver4
                 -optlm-mcpu=znver4
                 -fPIE
                 -fPIC
                 -- -optlo-O3
                 -optlo=--thinlto-bc
                 -optlc-O3
                 -optlas-O3
                 -optlm-O3
                 -- -optlc-mattr=+neon
                 -optlo-passes=default<O3>,load-store-vectorizer,slp-vectorizer,vector-combine,loop-vectorize,loop-idiom-vectorize

                 -- everything else
                 -opta-O3
                 -optl-O3
                 -opta-flto=thin
                 -optl-flto=thin
                 -optl-fuse-ld=lld
                 -fsplit-sections
                 -funbox-strict-fields
                 -- inline aggressively to ensure lenses/polymorphic functions are fully saturated at concrete types
                 -- we're about 50% slower on the hot path when -fexpose-all-unfoldings, -fspecialise-aggressively,
                 -- and -fpolymorphic-specialisation are turned off -- despite all the manual INLINE/SPECIALIZE pragmas.
                 -fexpose-all-unfoldings
                 -fspecialize-aggressively
                 -flate-specialise
                 -fpolymorphic-specialisation
                 -fstatic-argument-transformation
                 -fspec-eval
                 -fspec-eval-dictfun
                 -flate-dmd-anal
                 -fdmd-unbox-width=20
                 -fdicts-cheap
                 -fdo-clever-arg-eta-expansion
                 -- -flocal-float-out
                 -- -flocal-float-out-top-level
                 -- giving the specializer more room to chug makes the generated core more closely match the
                 -- source program. or at least, I can read it more easily and recognize what's happening
                 -- where.
                 -fmax-simplifier-iterations=128
                 -fsimplifier-phases=48
                 -fsimpl-tick-factor=3200
                 -funfolding-case-threshold=240
                 -funfolding-case-scaling=1600
                 -funfolding-use-threshold=1600
                 -funfolding-dict-discount=1600
                 -fmax-worker-args=20
                 -- results in a mild speedup, ~5%
                 -fworker-wrapper-cbv
                 -- -ddump-rules
                 -- -ddump-rule-firings
                 -- -ddump-rule-rewrites
                 -ddump-simpl
                 -ddump-to-file
                 -dno-typeable-binds
                 -dsuppress-coercions
                 -dsuppress-idinfo
                 -dsuppress-module-prefixes
                 -dsuppress-type-applications
                 -dsuppress-timestamps
                 -dsuppress-ticks
                 -ddump-splices
                 -- -fforce-recomp
                 -rtsopts
                 -- -fprof-late-overloaded-calls
                 -- -fprof-cafs
                 -- -threaded
                 -fomit-yields
